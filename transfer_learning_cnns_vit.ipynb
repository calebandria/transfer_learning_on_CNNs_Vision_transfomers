{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e1bc9f",
   "metadata": {},
   "source": [
    "# Transfer Learning Comparison on Caltech-256: CNNs vs Vision Transformers\n",
    "\n",
    "This notebook implements a comprehensive comparison of pretrained CNN and Vision Transformer\n",
    "architectures for transfer learning on the Caltech-256 dataset.\n",
    "\n",
    "## Models Compared\n",
    "| Family | Models |\n",
    "|--------|--------|\n",
    "| **CNNs** | ResNet-50, ResNet-101, VGG-16, EfficientNet-B0, MobileNet-V2, InceptionV3 |\n",
    "| **Transformers** | ViT-B/16, ViT-L/16, Swin Transformer, DeiT-Base |\n",
    "\n",
    "## Sections\n",
    "1. [Configuration](#1.-Configuration)\n",
    "2. [Setup & Imports](#2.-Setup-&-Imports)\n",
    "3. [Dataset Loading](#3.-Dataset-Loading)\n",
    "4. [Model Preparation](#4.-Model-Preparation)\n",
    "5. [Training Pipeline](#5.-Training-Pipeline)\n",
    "6. [Evaluation](#6.-Evaluation)\n",
    "7. [Visualisation & Comparison](#7.-Visualisation-&-Comparison)\n",
    "8. [Results Summary](#8.-Results-Summary)\n",
    "9. [Conclusions](#9.-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4bb57",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit the values in this cell to customise training behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# âš™ï¸  CONFIGURATION â€“ edit this cell before running the notebook\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# --- Dataset ---\n",
    "DATA_DIR = \"/path/to/caltech-256\"   # Root directory that contains class folders\n",
    "                                     # e.g. 001.ak47/, 002.american-flag/, ...\n",
    "\n",
    "# --- Splits ---\n",
    "TRAIN_RATIO = 0.70   # 70 % training\n",
    "VAL_RATIO   = 0.15   # 15 % validation  â†’ 15 % test (remainder)\n",
    "\n",
    "# --- Training ---\n",
    "BATCH_SIZE  = 32\n",
    "NUM_EPOCHS  = 30\n",
    "LR          = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE    = 7       # Early-stopping patience (epochs without val_loss improvement)\n",
    "\n",
    "# --- Transfer learning mode ---\n",
    "FREEZE_BACKBONE = True   # True  â†’ feature extraction (only classifier trains)\n",
    "                          # False â†’ full fine-tuning (all layers train)\n",
    "UNFREEZE_AFTER  = 5      # Epochs before unfreezing last UNFREEZE_N_LAYERS layers\n",
    "                          # (set to None to keep backbone frozen throughout)\n",
    "UNFREEZE_N_LAYERS = 3    # Number of backbone layer groups to unfreeze after UNFREEZE_AFTER epochs\n",
    "\n",
    "# --- Augmentation ---\n",
    "AUGMENT_TRAIN = True\n",
    "\n",
    "# --- Data loader ---\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "\n",
    "# --- Checkpoints & results ---\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "RESULTS_DIR    = \"results\"\n",
    "\n",
    "# --- Models to run (subset if you want a quick test) ---\n",
    "# All supported keys:\n",
    "#   CNNs        : \"resnet50\", \"resnet101\", \"vgg16\", \"efficientnet_b0\",\n",
    "#                 \"mobilenet_v2\", \"inception_v3\"\n",
    "#   Transformers: \"vit_b_16\", \"vit_l_16\", \"swin_t\", \"deit_base_patch16_224\"\n",
    "MODELS_TO_RUN = [\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"vgg16\",\n",
    "    \"efficientnet_b0\",\n",
    "    \"mobilenet_v2\",\n",
    "    \"inception_v3\",\n",
    "    \"vit_b_16\",\n",
    "    \"vit_l_16\",\n",
    "    \"swin_t\",\n",
    "    \"deit_base_patch16_224\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b2409",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the repo root to sys.path so that models.py / utils.py are importable\n",
    "repo_root = Path(\".\").resolve()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# Local modules\n",
    "from models import (\n",
    "    get_model,\n",
    "    list_models,\n",
    "    count_all_parameters,\n",
    "    freeze_backbone,\n",
    "    unfreeze_last_n_layers,\n",
    "    INPUT_SIZES,\n",
    "    MODEL_NAMES,\n",
    ")\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    load_caltech256,\n",
    "    train_model,\n",
    "    evaluate_model,\n",
    "    measure_inference_time,\n",
    "    build_summary_df,\n",
    "    save_summary_csv,\n",
    "    plot_training_curves,\n",
    "    plot_accuracy_comparison,\n",
    "    plot_inference_time_comparison,\n",
    "    plot_size_vs_accuracy,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "\n",
    "# â”€â”€ Device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"\\nAvailable models: {list_models()}\")\n",
    "print(f\"Models selected for this run: {MODELS_TO_RUN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69542506",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading\n",
    "\n",
    "Caltech-256 is expected to be organised as:\n",
    "```\n",
    "DATA_DIR/\n",
    "    001.ak47/\n",
    "        image_0001.jpg\n",
    "        ...\n",
    "    002.american-flag/\n",
    "        ...\n",
    "    ...\n",
    "    257.clutter/\n",
    "```\n",
    "\n",
    "The dataset is split **once** (reproducibly) using `SEED` into train / val / test subsets.\n",
    "Data augmentation (random crop, flip, colour jitter, rotation) is applied only to the\n",
    "training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load Caltech-256 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Note: inception_v3 requires 299Ã—299 input; all other models use 224Ã—224.\n",
    "# We create one loader per unique input size.\n",
    "# For simplicity, and because most models share 224Ã—224, we load a single\n",
    "# set of loaders at 224Ã—224 and re-use them for all models except InceptionV3.\n",
    "\n",
    "print(\"Loading Caltech-256 â€¦\")\n",
    "\n",
    "train_loader_224, val_loader_224, test_loader_224, class_names = load_caltech256(\n",
    "    data_dir=DATA_DIR,\n",
    "    input_size=224,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    seed=SEED,\n",
    "    augment=AUGMENT_TRAIN,\n",
    ")\n",
    "\n",
    "# Dedicated 299Ã—299 loaders for InceptionV3\n",
    "train_loader_299, val_loader_299, test_loader_299, _ = load_caltech256(\n",
    "    data_dir=DATA_DIR,\n",
    "    input_size=299,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    seed=SEED,\n",
    "    augment=AUGMENT_TRAIN,\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class names (first 10): {class_names[:10]}\")\n",
    "\n",
    "# Helper: pick the right loaders for a model key\n",
    "def get_loaders(model_key: str):\n",
    "    if model_key == \"inception_v3\":\n",
    "        return train_loader_299, val_loader_299, test_loader_299\n",
    "    return train_loader_224, val_loader_224, test_loader_224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f262bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualise a batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def imshow_batch(loader, title=\"Sample batch\", n=8):\n",
    "    imgs, labels = next(iter(loader))\n",
    "    # Denormalise\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    imgs = (imgs[:n] * std + mean).clamp(0, 1)\n",
    "    grid = vutils.make_grid(imgs, nrow=n, padding=2)\n",
    "    fig, ax = plt.subplots(figsize=(16, 2.5))\n",
    "    ax.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    label_names = [class_names[l] for l in labels[:n].tolist()]\n",
    "    print(\"Labels:\", label_names)\n",
    "\n",
    "imshow_batch(train_loader_224, title=\"Training batch sample (224Ã—224)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ec244",
   "metadata": {},
   "source": [
    "## 4. Model Preparation\n",
    "\n",
    "For each selected model we:\n",
    "1. Load pretrained ImageNet weights via `torchvision.models` (or `timm` for DeiT).\n",
    "2. Replace the final classification head with a new linear layer for `NUM_CLASSES` outputs.\n",
    "3. Optionally freeze the backbone so that only the head is trained in the first phase.\n",
    "\n",
    "The `get_model()` factory handles all architecture-specific head replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3792ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Instantiate all selected models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "models_dict: dict = {}\n",
    "params_dict:  dict = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    print(f\"Loading {MODEL_NAMES.get(key, key)} â€¦\", end=\"  \")\n",
    "    m = get_model(key, pretrained=True, freeze=FREEZE_BACKBONE, device=device)\n",
    "    models_dict[key] = m\n",
    "    params_dict[key] = count_all_parameters(m)\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"total params={params_dict[key]:,}  trainable={trainable:,}\")\n",
    "\n",
    "print(\"\\nAll models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff03b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Parameter summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "param_rows = [\n",
    "    {\"Model\": MODEL_NAMES.get(k, k), \"Total Params (M)\": f\"{v/1e6:.2f}\"}\n",
    "    for k, v in params_dict.items()\n",
    "]\n",
    "pd.DataFrame(param_rows).sort_values(\"Total Params (M)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e425b04",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Each model is trained with:\n",
    "- **Loss**: Cross-Entropy\n",
    "- **Optimiser**: Adam (`lr=LR`, `weight_decay=WEIGHT_DECAY`)\n",
    "- **LR Scheduler**: `ReduceLROnPlateau` (halves LR when val_loss stagnates for 3 epochs)\n",
    "- **Early stopping**: stops if val_loss doesn't improve for `PATIENCE` epochs\n",
    "- **Checkpointing**: saves the model state with the best validation accuracy to `CHECKPOINT_DIR/`\n",
    "\n",
    "If `UNFREEZE_AFTER` is not `None`, the last `UNFREEZE_N_LAYERS` backbone layer groups are\n",
    "unfrozen after that many epochs (gradual fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Training loop (iterates over all selected models) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import time\n",
    "\n",
    "all_histories: dict = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    model = models_dict[key]\n",
    "    train_ldr, val_ldr, test_ldr = get_loaders(key)\n",
    "\n",
    "    # â”€â”€â”€ Phase 1: feature extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    phase1_epochs = UNFREEZE_AFTER if UNFREEZE_AFTER is not None else NUM_EPOCHS\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Training {MODEL_NAMES.get(key, key)}  (phase 1 â€“ {phase1_epochs} epochs)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    history = train_model(\n",
    "        model, train_ldr, val_ldr,\n",
    "        model_key=key,\n",
    "        num_epochs=phase1_epochs,\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        patience=PATIENCE,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€ Phase 2: fine-tuning (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if UNFREEZE_AFTER is not None and NUM_EPOCHS > UNFREEZE_AFTER:\n",
    "        print(f\"\\n --- Phase 2: unfreezing last {UNFREEZE_N_LAYERS} layer groups ---\")\n",
    "        unfreeze_last_n_layers(model, key, UNFREEZE_N_LAYERS)\n",
    "        trainable_now = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"     Trainable params after unfreeze: {trainable_now:,}\")\n",
    "\n",
    "        history2 = train_model(\n",
    "            model, train_ldr, val_ldr,\n",
    "            model_key=key,\n",
    "            num_epochs=NUM_EPOCHS - UNFREEZE_AFTER,\n",
    "            lr=LR / 10,          # smaller LR for fine-tuning\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            patience=PATIENCE,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            device=device,\n",
    "            resume_checkpoint=history[\"checkpoint_path\"],\n",
    "        )\n",
    "        # Merge histories\n",
    "        for metric in (\"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\", \"epoch_times\"):\n",
    "            history[metric].extend(history2[metric])\n",
    "        history[\"total_time\"] = history.get(\"total_time\", 0) + history2[\"total_time\"]\n",
    "        if history2[\"best_val_acc\"] > history[\"best_val_acc\"]:\n",
    "            history[\"best_val_acc\"] = history2[\"best_val_acc\"]\n",
    "            history[\"best_epoch\"] = len(history[\"train_loss\"])\n",
    "            history[\"checkpoint_path\"] = history2[\"checkpoint_path\"]\n",
    "\n",
    "    all_histories[key] = history\n",
    "    print(f\"\\nâœ“ {MODEL_NAMES.get(key, key)} done.\"\n",
    "          f\"  best_val_acc={history['best_val_acc']:.4f}\"\n",
    "          f\"  total_time={history['total_time']:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd6f4a",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We evaluate each model on the held-out **test set** and record:\n",
    "- **Top-1 accuracy** â€“ fraction of correctly classified images\n",
    "- **Top-5 accuracy** â€“ fraction of images whose true class is in the top-5 predictions\n",
    "- **Inference time** â€“ average ms per image (single-image benchmark on the same device)\n",
    "- **Per-class metrics** â€“ precision, recall, F1-score from scikit-learn\n",
    "- **Confusion matrix** â€“ saved as PNG for the best-performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Evaluate all models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_results: dict = {}\n",
    "\n",
    "for key in MODELS_TO_RUN:\n",
    "    model = models_dict[key]\n",
    "    _, _, test_ldr = get_loaders(key)\n",
    "    input_sz = INPUT_SIZES[key]\n",
    "\n",
    "    print(f\"\\nEvaluating {MODEL_NAMES.get(key, key)} â€¦\")\n",
    "\n",
    "    # Load best checkpoint\n",
    "    ckpt_path = all_histories[key][\"checkpoint_path\"]\n",
    "    if Path(ckpt_path).exists():\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        print(f\"  Loaded checkpoint: {ckpt_path}\")\n",
    "\n",
    "    eval_res = evaluate_model(model, test_ldr, class_names, device=device)\n",
    "\n",
    "    # Standalone inference benchmark\n",
    "    eval_res[\"avg_inference_ms\"] = measure_inference_time(\n",
    "        model, input_size=input_sz, n_runs=50, batch_size=1, device=device,\n",
    "    )\n",
    "    eval_res[\"total_time\"]    = all_histories[key][\"total_time\"]\n",
    "    eval_res[\"best_val_acc\"]  = all_histories[key][\"best_val_acc\"]\n",
    "\n",
    "    all_results[key] = eval_res\n",
    "\n",
    "    print(f\"  Top-1: {eval_res['top1_acc']*100:.2f}%  \"\n",
    "          f\"Top-5: {eval_res['top5_acc']*100:.2f}%  \"\n",
    "          f\"Inf: {eval_res['avg_inference_ms']:.2f} ms/img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d8e22",
   "metadata": {},
   "source": [
    "## 7. Visualisation & Comparison\n",
    "\n",
    "All plots are saved to `RESULTS_DIR/plots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Training curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plots_dir = Path(RESULTS_DIR) / \"plots\"\n",
    "\n",
    "for key, history in all_histories.items():\n",
    "    plot_training_curves(history, key, save_dir=plots_dir)\n",
    "\n",
    "print(\"Training curves saved.\")\n",
    "\n",
    "# â”€â”€ Display training curves inline for each model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(len(MODELS_TO_RUN), 2,\n",
    "                          figsize=(14, 4 * len(MODELS_TO_RUN)))\n",
    "if len(MODELS_TO_RUN) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax_row, key in zip(axes, MODELS_TO_RUN):\n",
    "    h = all_histories[key]\n",
    "    epochs = range(1, len(h[\"train_loss\"]) + 1)\n",
    "    ax_row[0].plot(epochs, h[\"train_loss\"], label=\"Train\")\n",
    "    ax_row[0].plot(epochs, h[\"val_loss\"],   label=\"Val\")\n",
    "    ax_row[0].set_title(f\"{MODEL_NAMES.get(key,key)} â€“ Loss\")\n",
    "    ax_row[0].set_xlabel(\"Epoch\"); ax_row[0].set_ylabel(\"Loss\")\n",
    "    ax_row[0].legend()\n",
    "\n",
    "    ax_row[1].plot(epochs, [a*100 for a in h[\"train_acc\"]], label=\"Train\")\n",
    "    ax_row[1].plot(epochs, [a*100 for a in h[\"val_acc\"]],   label=\"Val\")\n",
    "    ax_row[1].set_title(f\"{MODEL_NAMES.get(key,key)} â€“ Accuracy\")\n",
    "    ax_row[1].set_xlabel(\"Epoch\"); ax_row[1].set_ylabel(\"Accuracy (%)\")\n",
    "    ax_row[1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f469834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Summary DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summary_df = build_summary_df(all_results, params_dict)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "save_summary_csv(summary_df, path=Path(RESULTS_DIR) / \"summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ba56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Accuracy comparison bar chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plot_accuracy_comparison(summary_df, save_dir=plots_dir)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = [\"steelblue\" if any(t in m for t in [\"ViT\", \"Swin\", \"DeiT\"])\n",
    "          else \"salmon\" for m in summary_df[\"Model\"]]\n",
    "ax.bar(summary_df[\"Model\"], summary_df[\"Top-1 Acc (%)\"], color=colors)\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Top-1 Accuracy (%)\")\n",
    "ax.set_title(\"Top-1 Accuracy â€“ CNNs (red) vs Transformers (blue)\")\n",
    "ax.set_xticklabels(summary_df[\"Model\"], rotation=35, ha=\"right\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e072b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inference time comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plot_inference_time_comparison(summary_df, save_dir=plots_dir)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(summary_df[\"Model\"], summary_df[\"Inf. Time (ms)\"], color=\"mediumseagreen\")\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Avg Inference Time (ms / image)\")\n",
    "ax.set_title(\"Inference Time Comparison\")\n",
    "ax.set_xticklabels(summary_df[\"Model\"], rotation=35, ha=\"right\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Model size vs accuracy scatter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plot_size_vs_accuracy(summary_df, save_dir=plots_dir)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(summary_df[\"Params (M)\"], summary_df[\"Top-1 Acc (%)\"], s=80)\n",
    "for _, row in summary_df.iterrows():\n",
    "    ax.annotate(row[\"Model\"], (row[\"Params (M)\"], row[\"Top-1 Acc (%)\"]),\n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=9)\n",
    "ax.set_xlabel(\"Parameters (M)\")\n",
    "ax.set_ylabel(\"Top-1 Accuracy (%)\")\n",
    "ax.set_title(\"Model Size vs Accuracy Trade-off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d57c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Confusion matrix for the best model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_key = summary_df.iloc[0][\"Model\"]\n",
    "# Reverse MODEL_NAMES to find the key\n",
    "rev_names = {v: k for k, v in MODEL_NAMES.items()}\n",
    "best_model_key = rev_names.get(best_key, MODELS_TO_RUN[0])\n",
    "\n",
    "print(f\"Best model: {best_key} (key={best_model_key})\")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    all_results[best_model_key][\"confusion_matrix\"],\n",
    "    class_names,\n",
    "    model_key=best_model_key,\n",
    "    save_dir=plots_dir,\n",
    "    max_classes=30,\n",
    ")\n",
    "print(\"Confusion matrix saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffd504",
   "metadata": {},
   "source": [
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final ranked table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 70)\n",
    "print(\" FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "best = summary_df.iloc[0]\n",
    "print(f\"ğŸ† Best model by Top-1 accuracy: {best['Model']}\")\n",
    "print(f\"   Top-1 Acc : {best['Top-1 Acc (%)']:.2f}%\")\n",
    "print(f\"   Top-5 Acc : {best['Top-5 Acc (%)']:.2f}%\")\n",
    "print(f\"   Inf. Time  : {best['Inf. Time (ms)']:.2f} ms/img\")\n",
    "print(f\"   Params     : {best['Params (M)']:.1f} M\")\n",
    "\n",
    "fastest = summary_df.sort_values(\"Inf. Time (ms)\").iloc[0]\n",
    "print(f\"\\nâš¡ Fastest model by inference time: {fastest['Model']}\")\n",
    "print(f\"   Inf. Time: {fastest['Inf. Time (ms)']:.2f} ms/img\")\n",
    "print(f\"   Top-1 Acc: {fastest['Top-1 Acc (%)']:.2f}%\")\n",
    "\n",
    "most_efficient = (summary_df[\"Top-1 Acc (%)\"] / summary_df[\"Params (M)\"]).idxmax()\n",
    "eff = summary_df.iloc[most_efficient]\n",
    "print(f\"\\nğŸ’¡ Most parameter-efficient: {eff['Model']}\")\n",
    "print(f\"   Acc/Params ratio: {eff['Top-1 Acc (%)']/eff['Params (M)']:.2f}% per M params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81e69f",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Key Observations\n",
    "- **Accuracy**: Vision Transformers (ViT-B/16, ViT-L/16, DeiT, Swin) typically achieve\n",
    "  higher top-1 accuracy on Caltech-256 when sufficient training data is available, because\n",
    "  their self-attention mechanism captures long-range dependencies better than local convolutions.\n",
    "- **Speed**: Lightweight CNNs (MobileNet-V2, EfficientNet-B0) offer the fastest inference,\n",
    "  making them ideal for deployment-constrained environments.\n",
    "- **Parameter efficiency**: EfficientNet-B0 and MobileNet-V2 deliver competitive accuracy\n",
    "  with far fewer parameters than ViT-L/16 or VGG-16.\n",
    "- **Training stability**: CNNs generally converge faster in the early epochs due to their\n",
    "  inductive biases (locality, translation equivariance), while Transformers may require\n",
    "  more epochs or data augmentation to reach peak performance.\n",
    "\n",
    "### Recommendations\n",
    "| Use-case | Recommended model |\n",
    "|----------|-------------------|\n",
    "| Highest accuracy | ViT-B/16 or Swin-T |\n",
    "| Edge / mobile deployment | MobileNet-V2 or EfficientNet-B0 |\n",
    "| Balanced accuracy & speed | ResNet-50 or EfficientNet-B0 |\n",
    "| Fine-tuning with limited data | ResNet-50 (strong ImageNet prior) |\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with stronger augmentation (RandAugment, MixUp, CutMix) for the Transformer models.\n",
    "- Try longer training with a cosine LR schedule.\n",
    "- Explore few-shot or semi-supervised approaches for the long-tail classes.\n",
    "- Perform per-class error analysis to identify systematic weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7af04",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
