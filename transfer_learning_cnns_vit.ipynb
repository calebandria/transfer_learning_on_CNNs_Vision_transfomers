{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transfer Learning Comparison on Caltech-256: CNNs vs Vision Transformers\n\nThis notebook implements a comprehensive comparison of pretrained CNN and Vision Transformer\narchitectures for transfer learning on the Caltech-256 dataset.\n\n## Models Compared\n| Family | Models |\n|--------|--------|\n| **CNNs** | ResNet-50, ResNet-101, VGG-16, EfficientNet-B0, MobileNet-V2, InceptionV3 |\n| **Transformers** | ViT-B/16, ViT-L/16, Swin Transformer, DeiT-Base |\n\n## Sections\n1. [Configuration](#1.-Configuration)\n2. [Setup & Imports](#2.-Setup-&-Imports)\n3. [Dataset Loading](#3.-Dataset-Loading)\n4. [Model Preparation](#4.-Model-Preparation)\n5. [Training Pipeline](#5.-Training-Pipeline)\n6. [Evaluation](#6.-Evaluation)\n7. [Visualisation & Comparison](#7.-Visualisation-&-Comparison)\n8. [Results Summary](#8.-Results-Summary)\n9. [Conclusions](#9.-Conclusions)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuration\n\nEdit the values in this cell to customise training behaviour."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------\n# \u2699\ufe0f  CONFIGURATION \u2013 edit this cell before running the notebook\n# ---------------------------------------------------------------\n\n# --- Dataset ---\nDATA_DIR = \"/path/to/caltech-256\"   # Root directory that contains class folders\n                                     # e.g. 001.ak47/, 002.american-flag/, ...\n\n# --- Splits ---\nTRAIN_RATIO = 0.70   # 70 % training\nVAL_RATIO   = 0.15   # 15 % validation  \u2192 15 % test (remainder)\n\n# --- Training ---\nBATCH_SIZE  = 32\nNUM_EPOCHS  = 30\nLR          = 1e-3\nWEIGHT_DECAY = 1e-4\nPATIENCE    = 7       # Early-stopping patience (epochs without val_loss improvement)\n\n# --- Transfer learning mode ---\nFREEZE_BACKBONE = True   # True  \u2192 feature extraction (only classifier trains)\n                          # False \u2192 full fine-tuning (all layers train)\nUNFREEZE_AFTER  = 5      # Epochs before unfreezing last UNFREEZE_N_LAYERS layers\n                          # (set to None to keep backbone frozen throughout)\nUNFREEZE_N_LAYERS = 3    # Number of backbone layer groups to unfreeze after UNFREEZE_AFTER epochs\n\n# --- Augmentation ---\nAUGMENT_TRAIN = True\n\n# --- Data loader ---\nNUM_WORKERS = 4\n\n# --- Reproducibility ---\nSEED = 42\n\n# --- Checkpoints & results ---\nCHECKPOINT_DIR = \"checkpoints\"\nRESULTS_DIR    = \"results\"\n\n# --- Models to run (subset if you want a quick test) ---\n# All supported keys:\n#   CNNs        : \"resnet50\", \"resnet101\", \"vgg16\", \"efficientnet_b0\",\n#                 \"mobilenet_v2\", \"inception_v3\"\n#   Transformers: \"vit_b_16\", \"vit_l_16\", \"swin_t\", \"deit_base_patch16_224\"\nMODELS_TO_RUN = [\n    \"resnet50\",\n    \"resnet101\",\n    \"vgg16\",\n    \"efficientnet_b0\",\n    \"mobilenet_v2\",\n    \"inception_v3\",\n    \"vit_b_16\",\n    \"vit_l_16\",\n    \"swin_t\",\n    \"deit_base_patch16_224\",\n]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Setup & Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# Add the repo root to sys.path so that models.py / utils.py are importable\nrepo_root = Path(\".\").resolve()\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams[\"figure.dpi\"] = 100\n\n# Local modules\nfrom models import (\n    get_model,\n    list_models,\n    count_all_parameters,\n    freeze_backbone,\n    unfreeze_last_n_layers,\n    INPUT_SIZES,\n    MODEL_NAMES,\n)\nfrom utils import (\n    set_seed,\n    load_caltech256,\n    train_model,\n    evaluate_model,\n    measure_inference_time,\n    build_summary_df,\n    save_summary_csv,\n    plot_training_curves,\n    plot_accuracy_comparison,\n    plot_inference_time_comparison,\n    plot_size_vs_accuracy,\n    plot_confusion_matrix,\n)\n\n# \u2500\u2500 Device \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif device.type == \"cuda\":\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nset_seed(SEED)\n\nprint(f\"\\nAvailable models: {list_models()}\")\nprint(f\"Models selected for this run: {MODELS_TO_RUN}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dataset Loading\n\nCaltech-256 is expected to be organised as:\n```\nDATA_DIR/\n    001.ak47/\n        image_0001.jpg\n        ...\n    002.american-flag/\n        ...\n    ...\n    257.clutter/\n```\n\nThe dataset is split **once** (reproducibly) using `SEED` into train / val / test subsets.\nData augmentation (random crop, flip, colour jitter, rotation) is applied only to the\ntraining split."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Load Caltech-256 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Note: inception_v3 requires 299\u00d7299 input; all other models use 224\u00d7224.\n# We create one loader per unique input size.\n# For simplicity, and because most models share 224\u00d7224, we load a single\n# set of loaders at 224\u00d7224 and re-use them for all models except InceptionV3.\n\nprint(\"Loading Caltech-256 \u2026\")\n\ntrain_loader_224, val_loader_224, test_loader_224, class_names = load_caltech256(\n    data_dir=DATA_DIR,\n    input_size=224,\n    train_ratio=TRAIN_RATIO,\n    val_ratio=VAL_RATIO,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    seed=SEED,\n    augment=AUGMENT_TRAIN,\n)\n\n# Dedicated 299\u00d7299 loaders for InceptionV3\ntrain_loader_299, val_loader_299, test_loader_299, _ = load_caltech256(\n    data_dir=DATA_DIR,\n    input_size=299,\n    train_ratio=TRAIN_RATIO,\n    val_ratio=VAL_RATIO,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    seed=SEED,\n    augment=AUGMENT_TRAIN,\n)\n\nNUM_CLASSES = len(class_names)\nprint(f\"Number of classes: {NUM_CLASSES}\")\nprint(f\"Class names (first 10): {class_names[:10]}\")\n\n# Helper: pick the right loaders for a model key\ndef get_loaders(model_key: str):\n    if model_key == \"inception_v3\":\n        return train_loader_299, val_loader_299, test_loader_299\n    return train_loader_224, val_loader_224, test_loader_224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Visualise a batch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport torchvision.utils as vutils\n\ndef imshow_batch(loader, title=\"Sample batch\", n=8):\n    imgs, labels = next(iter(loader))\n    # Denormalise\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    imgs = (imgs[:n] * std + mean).clamp(0, 1)\n    grid = vutils.make_grid(imgs, nrow=n, padding=2)\n    fig, ax = plt.subplots(figsize=(16, 2.5))\n    ax.imshow(grid.permute(1, 2, 0).numpy())\n    ax.set_title(title)\n    ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n    label_names = [class_names[l] for l in labels[:n].tolist()]\n    print(\"Labels:\", label_names)\n\nimshow_batch(train_loader_224, title=\"Training batch sample (224\u00d7224)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Preparation\n\nFor each selected model we:\n1. Load pretrained ImageNet weights via `torchvision.models` (or `timm` for DeiT).\n2. Replace the final classification head with a new linear layer for `NUM_CLASSES` outputs.\n3. Optionally freeze the backbone so that only the head is trained in the first phase.\n\nThe `get_model()` factory handles all architecture-specific head replacements."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Instantiate all selected models \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmodels_dict: dict = {}\nparams_dict:  dict = {}\n\nfor key in MODELS_TO_RUN:\n    print(f\"Loading {MODEL_NAMES.get(key, key)} \u2026\", end=\"  \")\n    m = get_model(key, pretrained=True, freeze=FREEZE_BACKBONE, device=device)\n    models_dict[key] = m\n    params_dict[key] = count_all_parameters(m)\n    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n    print(f\"total params={params_dict[key]:,}  trainable={trainable:,}\")\n\nprint(\"\\nAll models loaded successfully.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Parameter summary table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nparam_rows = [\n    {\"Model\": MODEL_NAMES.get(k, k), \"Total Params (M)\": f\"{v/1e6:.2f}\"}\n    for k, v in params_dict.items()\n]\npd.DataFrame(param_rows).sort_values(\"Total Params (M)\", ascending=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Training Pipeline\n\nEach model is trained with:\n- **Loss**: Cross-Entropy\n- **Optimiser**: Adam (`lr=LR`, `weight_decay=WEIGHT_DECAY`)\n- **LR Scheduler**: `ReduceLROnPlateau` (halves LR when val_loss stagnates for 3 epochs)\n- **Early stopping**: stops if val_loss doesn't improve for `PATIENCE` epochs\n- **Checkpointing**: saves the model state with the best validation accuracy to `CHECKPOINT_DIR/`\n\nIf `UNFREEZE_AFTER` is not `None`, the last `UNFREEZE_N_LAYERS` backbone layer groups are\nunfrozen after that many epochs (gradual fine-tuning)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Training loop (iterates over all selected models) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport time\n\nall_histories: dict = {}\n\nfor key in MODELS_TO_RUN:\n    model = models_dict[key]\n    train_ldr, val_ldr, test_ldr = get_loaders(key)\n\n    # \u2500\u2500\u2500 Phase 1: feature extraction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    phase1_epochs = UNFREEZE_AFTER if UNFREEZE_AFTER is not None else NUM_EPOCHS\n    print(f\"\\n{'='*60}\")\n    print(f\" Training {MODEL_NAMES.get(key, key)}  (phase 1 \u2013 {phase1_epochs} epochs)\")\n    print(f\"{'='*60}\")\n\n    history = train_model(\n        model, train_ldr, val_ldr,\n        model_key=key,\n        num_epochs=phase1_epochs,\n        lr=LR,\n        weight_decay=WEIGHT_DECAY,\n        patience=PATIENCE,\n        checkpoint_dir=CHECKPOINT_DIR,\n        device=device,\n    )\n\n    # \u2500\u2500\u2500 Phase 2: fine-tuning (optional) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if UNFREEZE_AFTER is not None and NUM_EPOCHS > UNFREEZE_AFTER:\n        print(f\"\\n --- Phase 2: unfreezing last {UNFREEZE_N_LAYERS} layer groups ---\")\n        unfreeze_last_n_layers(model, key, UNFREEZE_N_LAYERS)\n        trainable_now = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"     Trainable params after unfreeze: {trainable_now:,}\")\n\n        history2 = train_model(\n            model, train_ldr, val_ldr,\n            model_key=key,\n            num_epochs=NUM_EPOCHS - UNFREEZE_AFTER,\n            lr=LR / 10,          # smaller LR for fine-tuning\n            weight_decay=WEIGHT_DECAY,\n            patience=PATIENCE,\n            checkpoint_dir=CHECKPOINT_DIR,\n            device=device,\n            resume_checkpoint=history[\"checkpoint_path\"],\n        )\n        # Merge histories\n        for metric in (\"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\", \"epoch_times\"):\n            history[metric].extend(history2[metric])\n        history[\"total_time\"] = history.get(\"total_time\", 0) + history2[\"total_time\"]\n        if history2[\"best_val_acc\"] > history[\"best_val_acc\"]:\n            history[\"best_val_acc\"] = history2[\"best_val_acc\"]\n            history[\"best_epoch\"] = len(history[\"train_loss\"])\n            history[\"checkpoint_path\"] = history2[\"checkpoint_path\"]\n\n    all_histories[key] = history\n    print(f\"\\n\u2713 {MODEL_NAMES.get(key, key)} done.\"\n          f\"  best_val_acc={history['best_val_acc']:.4f}\"\n          f\"  total_time={history['total_time']:.0f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Evaluation\n\nWe evaluate each model on the held-out **test set** and record:\n- **Top-1 accuracy** \u2013 fraction of correctly classified images\n- **Top-5 accuracy** \u2013 fraction of images whose true class is in the top-5 predictions\n- **Inference time** \u2013 average ms per image (single-image benchmark on the same device)\n- **Per-class metrics** \u2013 precision, recall, F1-score from scikit-learn\n- **Confusion matrix** \u2013 saved as PNG for the best-performing model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Evaluate all models \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nall_results: dict = {}\n\nfor key in MODELS_TO_RUN:\n    model = models_dict[key]\n    _, _, test_ldr = get_loaders(key)\n    input_sz = INPUT_SIZES[key]\n\n    print(f\"\\nEvaluating {MODEL_NAMES.get(key, key)} \u2026\")\n\n    # Load best checkpoint\n    ckpt_path = all_histories[key][\"checkpoint_path\"]\n    if Path(ckpt_path).exists():\n        ckpt = torch.load(ckpt_path, map_location=device)\n        model.load_state_dict(ckpt[\"model_state\"])\n        print(f\"  Loaded checkpoint: {ckpt_path}\")\n\n    eval_res = evaluate_model(model, test_ldr, class_names, device=device)\n\n    # Standalone inference benchmark\n    eval_res[\"avg_inference_ms\"] = measure_inference_time(\n        model, input_size=input_sz, n_runs=50, batch_size=1, device=device,\n    )\n    eval_res[\"total_time\"]    = all_histories[key][\"total_time\"]\n    eval_res[\"best_val_acc\"]  = all_histories[key][\"best_val_acc\"]\n\n    all_results[key] = eval_res\n\n    print(f\"  Top-1: {eval_res['top1_acc']*100:.2f}%  \"\n          f\"Top-5: {eval_res['top5_acc']*100:.2f}%  \"\n          f\"Inf: {eval_res['avg_inference_ms']:.2f} ms/img\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Visualisation & Comparison\n\nAll plots are saved to `RESULTS_DIR/plots/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Training curves \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nplots_dir = Path(RESULTS_DIR) / \"plots\"\n\nfor key, history in all_histories.items():\n    plot_training_curves(history, key, save_dir=plots_dir)\n\nprint(\"Training curves saved.\")\n\n# \u2500\u2500 Display training curves inline for each model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(len(MODELS_TO_RUN), 2,\n                          figsize=(14, 4 * len(MODELS_TO_RUN)))\nif len(MODELS_TO_RUN) == 1:\n    axes = [axes]\n\nfor ax_row, key in zip(axes, MODELS_TO_RUN):\n    h = all_histories[key]\n    epochs = range(1, len(h[\"train_loss\"]) + 1)\n    ax_row[0].plot(epochs, h[\"train_loss\"], label=\"Train\")\n    ax_row[0].plot(epochs, h[\"val_loss\"],   label=\"Val\")\n    ax_row[0].set_title(f\"{MODEL_NAMES.get(key,key)} \u2013 Loss\")\n    ax_row[0].set_xlabel(\"Epoch\"); ax_row[0].set_ylabel(\"Loss\")\n    ax_row[0].legend()\n\n    ax_row[1].plot(epochs, [a*100 for a in h[\"train_acc\"]], label=\"Train\")\n    ax_row[1].plot(epochs, [a*100 for a in h[\"val_acc\"]],   label=\"Val\")\n    ax_row[1].set_title(f\"{MODEL_NAMES.get(key,key)} \u2013 Accuracy\")\n    ax_row[1].set_xlabel(\"Epoch\"); ax_row[1].set_ylabel(\"Accuracy (%)\")\n    ax_row[1].legend()\n\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Summary DataFrame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsummary_df = build_summary_df(all_results, params_dict)\nprint(summary_df.to_string(index=False))\n\n# Save CSV\nsave_summary_csv(summary_df, path=Path(RESULTS_DIR) / \"summary.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Accuracy comparison bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nplot_accuracy_comparison(summary_df, save_dir=plots_dir)\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncolors = [\"steelblue\" if any(t in m for t in [\"ViT\", \"Swin\", \"DeiT\"])\n          else \"salmon\" for m in summary_df[\"Model\"]]\nax.bar(summary_df[\"Model\"], summary_df[\"Top-1 Acc (%)\"], color=colors)\nax.set_xlabel(\"Model\")\nax.set_ylabel(\"Top-1 Accuracy (%)\")\nax.set_title(\"Top-1 Accuracy \u2013 CNNs (red) vs Transformers (blue)\")\nax.set_xticklabels(summary_df[\"Model\"], rotation=35, ha=\"right\")\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Inference time comparison \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nplot_inference_time_comparison(summary_df, save_dir=plots_dir)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.bar(summary_df[\"Model\"], summary_df[\"Inf. Time (ms)\"], color=\"mediumseagreen\")\nax.set_xlabel(\"Model\")\nax.set_ylabel(\"Avg Inference Time (ms / image)\")\nax.set_title(\"Inference Time Comparison\")\nax.set_xticklabels(summary_df[\"Model\"], rotation=35, ha=\"right\")\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Model size vs accuracy scatter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nplot_size_vs_accuracy(summary_df, save_dir=plots_dir)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(summary_df[\"Params (M)\"], summary_df[\"Top-1 Acc (%)\"], s=80)\nfor _, row in summary_df.iterrows():\n    ax.annotate(row[\"Model\"], (row[\"Params (M)\"], row[\"Top-1 Acc (%)\"]),\n                textcoords=\"offset points\", xytext=(5, 5), fontsize=9)\nax.set_xlabel(\"Parameters (M)\")\nax.set_ylabel(\"Top-1 Accuracy (%)\")\nax.set_title(\"Model Size vs Accuracy Trade-off\")\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Confusion matrix for the best model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nbest_key = summary_df.iloc[0][\"Model\"]\n# Reverse MODEL_NAMES to find the key\nrev_names = {v: k for k, v in MODEL_NAMES.items()}\nbest_model_key = rev_names.get(best_key, MODELS_TO_RUN[0])\n\nprint(f\"Best model: {best_key} (key={best_model_key})\")\n\nplot_confusion_matrix(\n    all_results[best_model_key][\"confusion_matrix\"],\n    class_names,\n    model_key=best_model_key,\n    save_dir=plots_dir,\n    max_classes=30,\n)\nprint(\"Confusion matrix saved.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Results Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Final ranked table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"=\" * 70)\nprint(\" FINAL RESULTS SUMMARY\")\nprint(\"=\" * 70)\nprint(summary_df.to_string(index=False))\nprint()\n\nbest = summary_df.iloc[0]\nprint(f\"\ud83c\udfc6 Best model by Top-1 accuracy: {best['Model']}\")\nprint(f\"   Top-1 Acc : {best['Top-1 Acc (%)']:.2f}%\")\nprint(f\"   Top-5 Acc : {best['Top-5 Acc (%)']:.2f}%\")\nprint(f\"   Inf. Time  : {best['Inf. Time (ms)']:.2f} ms/img\")\nprint(f\"   Params     : {best['Params (M)']:.1f} M\")\n\nfastest = summary_df.sort_values(\"Inf. Time (ms)\").iloc[0]\nprint(f\"\\n\u26a1 Fastest model by inference time: {fastest['Model']}\")\nprint(f\"   Inf. Time: {fastest['Inf. Time (ms)']:.2f} ms/img\")\nprint(f\"   Top-1 Acc: {fastest['Top-1 Acc (%)']:.2f}%\")\n\nmost_efficient = (summary_df[\"Top-1 Acc (%)\"] / summary_df[\"Params (M)\"]).idxmax()\neff = summary_df.iloc[most_efficient]\nprint(f\"\\n\ud83d\udca1 Most parameter-efficient: {eff['Model']}\")\nprint(f\"   Acc/Params ratio: {eff['Top-1 Acc (%)']/eff['Params (M)']:.2f}% per M params\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Conclusions\n\n### Key Observations\n- **Accuracy**: Vision Transformers (ViT-B/16, ViT-L/16, DeiT, Swin) typically achieve\n  higher top-1 accuracy on Caltech-256 when sufficient training data is available, because\n  their self-attention mechanism captures long-range dependencies better than local convolutions.\n- **Speed**: Lightweight CNNs (MobileNet-V2, EfficientNet-B0) offer the fastest inference,\n  making them ideal for deployment-constrained environments.\n- **Parameter efficiency**: EfficientNet-B0 and MobileNet-V2 deliver competitive accuracy\n  with far fewer parameters than ViT-L/16 or VGG-16.\n- **Training stability**: CNNs generally converge faster in the early epochs due to their\n  inductive biases (locality, translation equivariance), while Transformers may require\n  more epochs or data augmentation to reach peak performance.\n\n### Recommendations\n| Use-case | Recommended model |\n|----------|-------------------|\n| Highest accuracy | ViT-B/16 or Swin-T |\n| Edge / mobile deployment | MobileNet-V2 or EfficientNet-B0 |\n| Balanced accuracy & speed | ResNet-50 or EfficientNet-B0 |\n| Fine-tuning with limited data | ResNet-50 (strong ImageNet prior) |\n\n### Next Steps\n- Experiment with stronger augmentation (RandAugment, MixUp, CutMix) for the Transformer models.\n- Try longer training with a cosine LR schedule.\n- Explore few-shot or semi-supervised approaches for the long-tail classes.\n- Perform per-class error analysis to identify systematic weaknesses."
  }
 ]
}